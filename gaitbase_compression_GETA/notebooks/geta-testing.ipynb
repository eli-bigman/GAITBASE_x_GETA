{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"f4441533-4d5a-4c9e-8906-1df24bda96ec","cell_type":"markdown","source":"## Tutorial 1. ResNet18 on CIFAR10. \n\n\nIn this tutorial, we will show \n\n- How to end-to-end train and compress a ResNet18 from scratch on CIFAR10 to get a compressed ResNet18.\n- The compressed ResNet18 achives both **high performance** and **significant FLOPs and parameters reductions** than the full model. \n- The compressed ResNet18 **reduces about 92% parameters** to achieve **92.91% accuracy** only lower than the baseline by **0.11%**.\n- More detailed new HESSO optimizer setup. (Technical report regarding HESSO will be released on the early of 2024).","metadata":{}},{"id":"f926ee6c-fedd-4078-a644-994a4dce224e","cell_type":"markdown","source":"### Step 0. Clone repo and set up enviroment ","metadata":{}},{"id":"a3340930-7d39-435b-974e-2b09f594507e","cell_type":"code","source":"# Clone GETA repositories\n!rm -rf /kaggle/working/geta\n!git clone --branch pytorch-2.6-compatibility https://github.com/eli-bigman/geta.git\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T14:55:54.102822Z","iopub.execute_input":"2025-07-16T14:55:54.103061Z","iopub.status.idle":"2025-07-16T14:55:55.009450Z","shell.execute_reply.started":"2025-07-16T14:55:54.103037Z","shell.execute_reply":"2025-07-16T14:55:55.008456Z"}},"outputs":[{"name":"stdout","text":"Cloning into 'geta'...\nremote: Enumerating objects: 297, done.\u001b[K\nremote: Counting objects: 100% (297/297), done.\u001b[K\nremote: Compressing objects: 100% (211/211), done.\u001b[K\nremote: Total 297 (delta 100), reused 265 (delta 76), pack-reused 0 (from 0)\u001b[K\nReceiving objects: 100% (297/297), 542.17 KiB | 12.91 MiB/s, done.\nResolving deltas: 100% (100/100), done.\n","output_type":"stream"}],"execution_count":1},{"id":"c2db9712-ae03-4998-a08f-ada88ce230bc","cell_type":"code","source":"!ls /kaggle/working/geta/sanity_check/backends\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T14:55:55.010549Z","iopub.execute_input":"2025-07-16T14:55:55.010869Z","iopub.status.idle":"2025-07-16T14:55:55.134507Z","shell.execute_reply.started":"2025-07-16T14:55:55.010831Z","shell.execute_reply":"2025-07-16T14:55:55.133639Z"}},"outputs":[{"name":"stdout","text":"carn\t\t\t\t   diffusion_transformer_sr\nconvnext.py\t\t\t   hf_llama\ndemo_group_conv_case1.py\t   hf_phi2\ndemonet_batchnorm_pruning.py\t   hf_sam\ndemonet_concat_case1.py\t\t   hf_vit\ndemonet_concat_case2.py\t\t   __init__.py\ndemonet_convtranspose_in_case1.py  mamba\ndemonet_convtranspose_in_case2.py  mlp.py\ndemonet_groupnorm_case1.py\t   resnet20_cifar10.py\ndemonet_groupnorm_case2.py\t   resnet_cifar10.py\ndemonet_groupnorm_case3.py\t   resnet_DuBIN.py\ndemonet_groupnorm_case4.py\t   resnet_DuBN.py\ndemonet_in_case3.py\t\t   simple_vit.py\ndemonet_weightshare_case1.py\t   tnlg\ndemonet_weightshare_case2.py\t   vgg7.py\ndensenet.py\t\t\t   vision_transformer\ndiffusion\n","output_type":"stream"}],"execution_count":2},{"id":"ad63c4f6-8ee3-4945-993a-2a2d1f118de6","cell_type":"code","source":"!pip install \"torch==2.0.1+cu117\" \\\n             \"torchvision==0.15.2+cu117\" \\\n             \"torchaudio==2.0.2\" \\\n             --extra-index-url https://download.pytorch.org/whl/cu117\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T14:55:55.136741Z","iopub.execute_input":"2025-07-16T14:55:55.137006Z","iopub.status.idle":"2025-07-16T14:57:36.104569Z","shell.execute_reply.started":"2025-07-16T14:55:55.136984Z","shell.execute_reply":"2025-07-16T14:57:36.103914Z"}},"outputs":[{"name":"stdout","text":"Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu117\nCollecting torch==2.0.1+cu117\n  Downloading https://download.pytorch.org/whl/cu117/torch-2.0.1%2Bcu117-cp311-cp311-linux_x86_64.whl (1843.9 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 GB\u001b[0m \u001b[31m480.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting torchvision==0.15.2+cu117\n  Downloading https://download.pytorch.org/whl/cu117/torchvision-0.15.2%2Bcu117-cp311-cp311-linux_x86_64.whl (6.1 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.1/6.1 MB\u001b[0m \u001b[31m92.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hCollecting torchaudio==2.0.2\n  Downloading https://download.pytorch.org/whl/cu117/torchaudio-2.0.2%2Bcu117-cp311-cp311-linux_x86_64.whl (4.4 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m85.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1+cu117) (3.18.0)\nRequirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1+cu117) (4.14.0)\nRequirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1+cu117) (1.13.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1+cu117) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1+cu117) (3.1.6)\nCollecting triton==2.0.0 (from torch==2.0.1+cu117)\n  Downloading https://download.pytorch.org/whl/triton-2.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision==0.15.2+cu117) (1.26.4)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torchvision==0.15.2+cu117) (2.32.4)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision==0.15.2+cu117) (11.2.1)\nRequirement already satisfied: cmake in /usr/local/lib/python3.11/dist-packages (from triton==2.0.0->torch==2.0.1+cu117) (3.31.6)\nCollecting lit (from triton==2.0.0->torch==2.0.1+cu117)\n  Downloading lit-18.1.8-py3-none-any.whl.metadata (2.5 kB)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.0.1+cu117) (3.0.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision==0.15.2+cu117) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision==0.15.2+cu117) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision==0.15.2+cu117) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision==0.15.2+cu117) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision==0.15.2+cu117) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision==0.15.2+cu117) (2.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision==0.15.2+cu117) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision==0.15.2+cu117) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision==0.15.2+cu117) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision==0.15.2+cu117) (2025.6.15)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.0.1+cu117) (1.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision==0.15.2+cu117) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision==0.15.2+cu117) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->torchvision==0.15.2+cu117) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->torchvision==0.15.2+cu117) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->torchvision==0.15.2+cu117) (2024.2.0)\nDownloading lit-18.1.8-py3-none-any.whl (96 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.4/96.4 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: lit, triton, torch, torchvision, torchaudio\n  Attempting uninstall: triton\n    Found existing installation: triton 3.2.0\n    Uninstalling triton-3.2.0:\n      Successfully uninstalled triton-3.2.0\n  Attempting uninstall: torch\n    Found existing installation: torch 2.6.0+cu124\n    Uninstalling torch-2.6.0+cu124:\n      Successfully uninstalled torch-2.6.0+cu124\n  Attempting uninstall: torchvision\n    Found existing installation: torchvision 0.21.0+cu124\n    Uninstalling torchvision-0.21.0+cu124:\n      Successfully uninstalled torchvision-0.21.0+cu124\n  Attempting uninstall: torchaudio\n    Found existing installation: torchaudio 2.6.0+cu124\n    Uninstalling torchaudio-2.6.0+cu124:\n      Successfully uninstalled torchaudio-2.6.0+cu124\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npytorch-lightning 2.5.2 requires torch>=2.1.0, but you have torch 2.0.1+cu117 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed lit-18.1.8 torch-2.0.1+cu117 torchaudio-2.0.2+cu117 torchvision-0.15.2+cu117 triton-2.0.0\n","output_type":"stream"}],"execution_count":3},{"id":"89587885-ceaa-4dc8-95b6-2c0ed4097821","cell_type":"code","source":"!python --version\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T14:57:36.105382Z","iopub.execute_input":"2025-07-16T14:57:36.105608Z","iopub.status.idle":"2025-07-16T14:57:36.228512Z","shell.execute_reply.started":"2025-07-16T14:57:36.105585Z","shell.execute_reply":"2025-07-16T14:57:36.227669Z"}},"outputs":[{"name":"stdout","text":"Python 3.11.13\n","output_type":"stream"}],"execution_count":4},{"id":"8067c3a6-bf56-4c85-96fb-2d4da488680b","cell_type":"markdown","source":"### Step 1. Create OTO instance","metadata":{}},{"id":"f2ce0399-51c7-4afc-b0f0-35bda3698825","cell_type":"code","source":"import sys\nsys.path.append('/kaggle/working/geta')\n# sys.path.append('/kaggle/working/OpenGait')\n# sys.path.append('..')\nfrom sanity_check.backends.resnet_cifar10 import resnet18_cifar10\nfrom only_train_once import OTO\nimport torch\n\nmodel = resnet18_cifar10()\ndummy_input = torch.rand(1, 3, 32, 32)\noto = OTO(model=model.cuda(), dummy_input=dummy_input.cuda())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T14:57:36.229500Z","iopub.execute_input":"2025-07-16T14:57:36.229709Z","iopub.status.idle":"2025-07-16T14:57:40.757271Z","shell.execute_reply.started":"2025-07-16T14:57:36.229687Z","shell.execute_reply":"2025-07-16T14:57:40.756688Z"}},"outputs":[{"name":"stdout","text":"OTO graph constructor\ngraph build\nNodePattern mul None\nNodePattern transpose None\nNodePattern matmul None\nPost-processing of graph completed.\nGraph has 70 nodes and 77 edges.\n","output_type":"stream"}],"execution_count":5},{"id":"01639056-422d-4bfb-89db-ff9f0725e807","cell_type":"markdown","source":"#### (Optional) Visualize the pruning dependancy graph of DNN","metadata":{}},{"id":"3e8cefb1-85cf-4833-a6ff-ee2f9270f02d","cell_type":"code","source":"# A ResNet_zig.gv.pdf will be generated to display the depandancy graph.\noto.visualize(view=False, out_dir='../cache')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T14:57:40.757912Z","iopub.execute_input":"2025-07-16T14:57:40.758162Z","iopub.status.idle":"2025-07-16T14:57:40.893121Z","shell.execute_reply.started":"2025-07-16T14:57:40.758145Z","shell.execute_reply":"2025-07-16T14:57:40.892536Z"}},"outputs":[{"name":"stderr","text":"Warning: #7c5 is not a known color.\n","output_type":"stream"}],"execution_count":6},{"id":"fc99fc48-fed7-41a5-ab6d-f2e5686f2003","cell_type":"markdown","source":"### Step 2. Dataset Preparation","metadata":{}},{"id":"57ec145d-4847-4f4b-8c12-782beb61cc61","cell_type":"code","source":"from torchvision.datasets import CIFAR10\nimport torchvision.transforms as transforms\n\ntrainset = CIFAR10(root='cifar10', train=True, download=True, transform=transforms.Compose([\n            transforms.RandomHorizontalFlip(),\n            transforms.RandomCrop(32, 4),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])]))\ntestset = CIFAR10(root='cifar10', train=False, download=True, transform=transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])]))\n\ntrainloader =  torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True, num_workers=4)\ntestloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False, num_workers=4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T14:57:40.893819Z","iopub.execute_input":"2025-07-16T14:57:40.894090Z","iopub.status.idle":"2025-07-16T14:57:49.578523Z","shell.execute_reply.started":"2025-07-16T14:57:40.894066Z","shell.execute_reply":"2025-07-16T14:57:49.577785Z"}},"outputs":[{"name":"stdout","text":"Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to cifar10/cifar-10-python.tar.gz\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 170498071/170498071 [00:04<00:00, 35115684.99it/s]\n","output_type":"stream"},{"name":"stdout","text":"Extracting cifar10/cifar-10-python.tar.gz to cifar10\nFiles already downloaded and verified\n","output_type":"stream"}],"execution_count":7},{"id":"65233349-da36-47bc-9af8-41a3f2d4429a","cell_type":"markdown","source":"### Step 3. Setup HESSO optimizer\n\nThe following main hyperparameters need to be taken care.\n\n- `variant`: The optimizer that is used for training the baseline full model. Currently support `sgd`, `adam` and `adamw`.\n- `lr`: The initial learning rate.\n- `weight_decay`: Weight decay as standard DNN optimization.\n- `target_group_sparsity`: The target group sparsity, typically higher group sparsity refers to more FLOPs and model size reduction, meanwhile may regress model performance more.\n- `start_pruning_steps`: The number of steps that **starts** to prune.\n- `pruning_steps`: The number of steps that **finishes** pruning (reach `target_group_sparsity`) after `start_pruning_steps`.\n- `pruning_periods`:  Incrementally produce the group sparsity equally among pruning periods.\n\nWe empirically suggest `start_pruning_steps` as 1/10 of total number of training steps. `pruning_steps` until 1/4 or 1/5 of total number of training steps.\nThe advatnages of HESSO compared to DHSPG is its explicit control over group sparsity exploration, which is typically more convenient.","metadata":{}},{"id":"48af4116-bb7e-4156-bc3d-c32c8fc6f2c0","cell_type":"code","source":"optimizer = oto.hesso(\n    variant='sgd', \n    lr=0.1, \n    weight_decay=1e-4,\n    target_group_sparsity=0.7,\n    start_pruning_step=10 * len(trainloader), \n    pruning_periods=10,\n    pruning_steps=10 * len(trainloader)\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T14:57:49.579360Z","iopub.execute_input":"2025-07-16T14:57:49.579634Z","iopub.status.idle":"2025-07-16T14:57:49.585222Z","shell.execute_reply.started":"2025-07-16T14:57:49.579609Z","shell.execute_reply":"2025-07-16T14:57:49.584414Z"}},"outputs":[{"name":"stdout","text":"Setup HESSO\nTarget redundant groups per period:  [201, 201, 201, 201, 201, 201, 201, 201, 201, 206]\n","output_type":"stream"}],"execution_count":8},{"id":"3632cb7e-e1ae-460c-b415-81c158c80a20","cell_type":"markdown","source":"### Step 4. Train ResNet18 as normal.","metadata":{}},{"id":"ed9d98a1-a77b-4cc6-9acd-a1483c17e370","cell_type":"code","source":"from tutorials.utils.utils import check_accuracy\n\nmax_epoch = 100\nmodel.cuda()\ncriterion = torch.nn.CrossEntropyLoss()\n# Every 50 epochs, decay lr by 10.0\nlr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.1) \n\nfor epoch in range(max_epoch):\n    f_avg_val = 0.0\n    model.train()\n    lr_scheduler.step()\n    for X, y in trainloader:\n        X = X.cuda()\n        y = y.cuda()\n        y_pred = model.forward(X)\n        f = criterion(y_pred, y)\n        optimizer.zero_grad()\n        f.backward()\n        f_avg_val += f\n        optimizer.step()\n    opt_metrics = optimizer.compute_metrics()\n    # group_sparsity, param_norm, _ = optimizer.compute_group_sparsity_param_norm()\n    # norm_important, norm_redundant, num_grps_important, num_grps_redundant = optimizer.compute_norm_groups()\n    accuracy1, accuracy5 = check_accuracy(model, testloader)\n    f_avg_val = f_avg_val.cpu().item() / len(trainloader)\n    \n    print(\"Ep: {ep}, loss: {f:.2f}, norm_all:{param_norm:.2f}, grp_sparsity: {gs:.2f}, acc1: {acc1:.4f}, norm_import: {norm_import:.2f}, norm_redund: {norm_redund:.2f}, num_grp_import: {num_grps_import}, num_grp_redund: {num_grps_redund}\"\\\n         .format(ep=epoch, f=f_avg_val, param_norm=opt_metrics.norm_params, gs=opt_metrics.group_sparsity, acc1=accuracy1,\\\n         norm_import=opt_metrics.norm_important_groups, norm_redund=opt_metrics.norm_redundant_groups, \\\n         num_grps_import=opt_metrics.num_important_groups, num_grps_redund=opt_metrics.num_redundant_groups\n        ))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T14:57:49.587341Z","iopub.execute_input":"2025-07-16T14:57:49.587996Z","iopub.status.idle":"2025-07-16T15:54:20.401092Z","shell.execute_reply.started":"2025-07-16T14:57:49.587973Z","shell.execute_reply":"2025-07-16T15:54:20.400007Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n","output_type":"stream"},{"name":"stdout","text":"Ep: 0, loss: 1.59, norm_all:4122.36, grp_sparsity: 0.00, acc1: 39.6700, norm_import: 4122.36, norm_redund: 0.00, num_grp_import: 2880, num_grp_redund: 0\nEp: 1, loss: 1.05, norm_all:4114.05, grp_sparsity: 0.00, acc1: 52.3600, norm_import: 4114.05, norm_redund: 0.00, num_grp_import: 2880, num_grp_redund: 0\nEp: 2, loss: 0.80, norm_all:4105.77, grp_sparsity: 0.00, acc1: 67.2000, norm_import: 4105.77, norm_redund: 0.00, num_grp_import: 2880, num_grp_redund: 0\nEp: 3, loss: 0.65, norm_all:4096.03, grp_sparsity: 0.00, acc1: 72.0100, norm_import: 4096.03, norm_redund: 0.00, num_grp_import: 2880, num_grp_redund: 0\nEp: 4, loss: 0.56, norm_all:4085.63, grp_sparsity: 0.00, acc1: 65.4200, norm_import: 4085.63, norm_redund: 0.00, num_grp_import: 2880, num_grp_redund: 0\nEp: 5, loss: 0.50, norm_all:4074.52, grp_sparsity: 0.00, acc1: 70.7400, norm_import: 4074.52, norm_redund: 0.00, num_grp_import: 2880, num_grp_redund: 0\nEp: 6, loss: 0.45, norm_all:4062.94, grp_sparsity: 0.00, acc1: 78.8300, norm_import: 4062.94, norm_redund: 0.00, num_grp_import: 2880, num_grp_redund: 0\nEp: 7, loss: 0.40, norm_all:4051.38, grp_sparsity: 0.00, acc1: 77.7100, norm_import: 4051.38, norm_redund: 0.00, num_grp_import: 2880, num_grp_redund: 0\nEp: 8, loss: 0.37, norm_all:4039.05, grp_sparsity: 0.00, acc1: 83.5100, norm_import: 4039.05, norm_redund: 0.00, num_grp_import: 2880, num_grp_redund: 0\nEp: 9, loss: 0.35, norm_all:4026.94, grp_sparsity: 0.00, acc1: 83.2400, norm_import: 4026.94, norm_redund: 0.00, num_grp_import: 2880, num_grp_redund: 0\nEp: 10, loss: 0.33, norm_all:3796.46, grp_sparsity: 0.07, acc1: 83.2900, norm_import: 3796.46, norm_redund: 0.00, num_grp_import: 2679, num_grp_redund: 201\nEp: 11, loss: 0.32, norm_all:3568.23, grp_sparsity: 0.14, acc1: 77.1000, norm_import: 3568.23, norm_redund: 0.00, num_grp_import: 2478, num_grp_redund: 402\nEp: 12, loss: 0.30, norm_all:3343.35, grp_sparsity: 0.21, acc1: 84.5000, norm_import: 3343.35, norm_redund: 0.00, num_grp_import: 2277, num_grp_redund: 603\nEp: 13, loss: 0.30, norm_all:3119.70, grp_sparsity: 0.28, acc1: 77.9800, norm_import: 3119.70, norm_redund: 0.00, num_grp_import: 2076, num_grp_redund: 804\nEp: 14, loss: 0.29, norm_all:2854.87, grp_sparsity: 0.35, acc1: 70.9800, norm_import: 2854.87, norm_redund: 0.00, num_grp_import: 1875, num_grp_redund: 1005\nEp: 15, loss: 0.28, norm_all:2555.40, grp_sparsity: 0.42, acc1: 68.8500, norm_import: 2555.40, norm_redund: 0.00, num_grp_import: 1674, num_grp_redund: 1206\nEp: 16, loss: 0.29, norm_all:2259.00, grp_sparsity: 0.49, acc1: 79.4600, norm_import: 2259.00, norm_redund: 0.00, num_grp_import: 1473, num_grp_redund: 1407\nEp: 17, loss: 0.30, norm_all:1951.52, grp_sparsity: 0.56, acc1: 79.0200, norm_import: 1951.52, norm_redund: 0.00, num_grp_import: 1272, num_grp_redund: 1608\nEp: 18, loss: 0.31, norm_all:1663.12, grp_sparsity: 0.63, acc1: 85.5000, norm_import: 1663.12, norm_redund: 0.00, num_grp_import: 1071, num_grp_redund: 1809\nEp: 19, loss: 0.35, norm_all:1365.22, grp_sparsity: 0.70, acc1: 78.4200, norm_import: 1365.22, norm_redund: 0.00, num_grp_import: 865, num_grp_redund: 2015\nEp: 20, loss: 0.36, norm_all:1378.28, grp_sparsity: 0.70, acc1: 82.5500, norm_import: 1378.28, norm_redund: 0.00, num_grp_import: 865, num_grp_redund: 2015\nEp: 21, loss: 0.31, norm_all:1387.53, grp_sparsity: 0.70, acc1: 87.8500, norm_import: 1387.53, norm_redund: 0.00, num_grp_import: 865, num_grp_redund: 2015\nEp: 22, loss: 0.28, norm_all:1394.87, grp_sparsity: 0.70, acc1: 85.6200, norm_import: 1394.87, norm_redund: 0.00, num_grp_import: 865, num_grp_redund: 2015\nEp: 23, loss: 0.27, norm_all:1401.32, grp_sparsity: 0.70, acc1: 76.9800, norm_import: 1401.32, norm_redund: 0.00, num_grp_import: 865, num_grp_redund: 2015\nEp: 24, loss: 0.25, norm_all:1407.42, grp_sparsity: 0.70, acc1: 82.4600, norm_import: 1407.42, norm_redund: 0.00, num_grp_import: 865, num_grp_redund: 2015\nEp: 25, loss: 0.23, norm_all:1412.81, grp_sparsity: 0.70, acc1: 75.3900, norm_import: 1412.81, norm_redund: 0.00, num_grp_import: 865, num_grp_redund: 2015\nEp: 26, loss: 0.23, norm_all:1418.06, grp_sparsity: 0.70, acc1: 85.5500, norm_import: 1418.06, norm_redund: 0.00, num_grp_import: 865, num_grp_redund: 2015\nEp: 27, loss: 0.22, norm_all:1422.79, grp_sparsity: 0.70, acc1: 72.1400, norm_import: 1422.79, norm_redund: 0.00, num_grp_import: 865, num_grp_redund: 2015\nEp: 28, loss: 0.20, norm_all:1427.09, grp_sparsity: 0.70, acc1: 85.9400, norm_import: 1427.09, norm_redund: 0.00, num_grp_import: 865, num_grp_redund: 2015\nEp: 29, loss: 0.19, norm_all:1431.06, grp_sparsity: 0.70, acc1: 88.1900, norm_import: 1431.06, norm_redund: 0.00, num_grp_import: 865, num_grp_redund: 2015\nEp: 30, loss: 0.19, norm_all:1434.86, grp_sparsity: 0.70, acc1: 88.8100, norm_import: 1434.86, norm_redund: 0.00, num_grp_import: 865, num_grp_redund: 2015\nEp: 31, loss: 0.18, norm_all:1438.19, grp_sparsity: 0.70, acc1: 88.8100, norm_import: 1438.19, norm_redund: 0.00, num_grp_import: 865, num_grp_redund: 2015\nEp: 32, loss: 0.17, norm_all:1441.43, grp_sparsity: 0.70, acc1: 86.0100, norm_import: 1441.43, norm_redund: 0.00, num_grp_import: 865, num_grp_redund: 2015\nEp: 33, loss: 0.16, norm_all:1443.96, grp_sparsity: 0.70, acc1: 85.8700, norm_import: 1443.96, norm_redund: 0.00, num_grp_import: 865, num_grp_redund: 2015\nEp: 34, loss: 0.16, norm_all:1446.58, grp_sparsity: 0.70, acc1: 89.7000, norm_import: 1446.58, norm_redund: 0.00, num_grp_import: 865, num_grp_redund: 2015\nEp: 35, loss: 0.15, norm_all:1448.74, grp_sparsity: 0.70, acc1: 86.1700, norm_import: 1448.74, norm_redund: 0.00, num_grp_import: 865, num_grp_redund: 2015\nEp: 36, loss: 0.15, norm_all:1451.74, grp_sparsity: 0.70, acc1: 87.5200, norm_import: 1451.74, norm_redund: 0.00, num_grp_import: 865, num_grp_redund: 2015\nEp: 37, loss: 0.14, norm_all:1454.32, grp_sparsity: 0.70, acc1: 88.9000, norm_import: 1454.32, norm_redund: 0.00, num_grp_import: 865, num_grp_redund: 2015\nEp: 38, loss: 0.14, norm_all:1456.63, grp_sparsity: 0.70, acc1: 89.3200, norm_import: 1456.63, norm_redund: 0.00, num_grp_import: 865, num_grp_redund: 2015\nEp: 39, loss: 0.13, norm_all:1458.59, grp_sparsity: 0.70, acc1: 90.0600, norm_import: 1458.59, norm_redund: 0.00, num_grp_import: 865, num_grp_redund: 2015\nEp: 40, loss: 0.13, norm_all:1460.97, grp_sparsity: 0.70, acc1: 80.0000, norm_import: 1460.97, norm_redund: 0.00, num_grp_import: 865, num_grp_redund: 2015\nEp: 41, loss: 0.13, norm_all:1462.77, grp_sparsity: 0.70, acc1: 90.3400, norm_import: 1462.77, norm_redund: 0.00, num_grp_import: 865, num_grp_redund: 2015\nEp: 42, loss: 0.12, norm_all:1464.56, grp_sparsity: 0.70, acc1: 89.7900, norm_import: 1464.56, norm_redund: 0.00, num_grp_import: 865, num_grp_redund: 2015\nEp: 43, loss: 0.12, norm_all:1465.77, grp_sparsity: 0.70, acc1: 86.7600, norm_import: 1465.77, norm_redund: 0.00, num_grp_import: 865, num_grp_redund: 2015\nEp: 44, loss: 0.11, norm_all:1466.86, grp_sparsity: 0.70, acc1: 87.2100, norm_import: 1466.86, norm_redund: 0.00, num_grp_import: 865, num_grp_redund: 2015\nEp: 45, loss: 0.11, norm_all:1468.12, grp_sparsity: 0.70, acc1: 90.4700, norm_import: 1468.12, norm_redund: 0.00, num_grp_import: 865, num_grp_redund: 2015\nEp: 46, loss: 0.11, norm_all:1469.94, grp_sparsity: 0.70, acc1: 88.1100, norm_import: 1469.94, norm_redund: 0.00, num_grp_import: 865, num_grp_redund: 2015\nEp: 47, loss: 0.10, norm_all:1470.70, grp_sparsity: 0.70, acc1: 90.0300, norm_import: 1470.70, norm_redund: 0.00, num_grp_import: 865, num_grp_redund: 2015\nEp: 48, loss: 0.10, norm_all:1472.33, grp_sparsity: 0.70, acc1: 87.1000, norm_import: 1472.33, norm_redund: 0.00, num_grp_import: 865, num_grp_redund: 2015\nEp: 49, loss: 0.07, norm_all:1471.35, grp_sparsity: 0.70, acc1: 92.1700, norm_import: 1471.35, norm_redund: 0.00, num_grp_import: 865, num_grp_redund: 2015\nEp: 50, loss: 0.05, norm_all:1470.37, grp_sparsity: 0.70, acc1: 92.3600, norm_import: 1470.37, norm_redund: 0.00, num_grp_import: 865, num_grp_redund: 2015\nEp: 51, loss: 0.04, norm_all:1469.39, grp_sparsity: 0.70, acc1: 92.5000, norm_import: 1469.39, norm_redund: 0.00, num_grp_import: 865, num_grp_redund: 2015\nEp: 52, loss: 0.04, norm_all:1468.39, grp_sparsity: 0.70, acc1: 92.5300, norm_import: 1468.39, norm_redund: 0.00, num_grp_import: 865, num_grp_redund: 2015\nEp: 53, loss: 0.04, norm_all:1467.39, grp_sparsity: 0.70, acc1: 92.5300, norm_import: 1467.39, norm_redund: 0.00, num_grp_import: 865, num_grp_redund: 2015\nEp: 54, loss: 0.03, norm_all:1466.39, grp_sparsity: 0.70, acc1: 92.5500, norm_import: 1466.39, norm_redund: 0.00, num_grp_import: 865, num_grp_redund: 2015\nEp: 55, loss: 0.03, norm_all:1465.38, grp_sparsity: 0.70, acc1: 92.4100, norm_import: 1465.38, norm_redund: 0.00, num_grp_import: 865, num_grp_redund: 2015\nEp: 56, loss: 0.03, norm_all:1464.37, grp_sparsity: 0.70, acc1: 92.5700, norm_import: 1464.37, norm_redund: 0.00, num_grp_import: 865, num_grp_redund: 2015\nEp: 57, loss: 0.03, norm_all:1463.36, grp_sparsity: 0.70, acc1: 92.4200, norm_import: 1463.36, norm_redund: 0.00, num_grp_import: 865, num_grp_redund: 2015\nEp: 58, loss: 0.03, norm_all:1462.34, grp_sparsity: 0.70, acc1: 92.5600, norm_import: 1462.34, norm_redund: 0.00, num_grp_import: 865, num_grp_redund: 2015\nEp: 59, loss: 0.03, norm_all:1461.33, grp_sparsity: 0.70, acc1: 92.7400, norm_import: 1461.33, norm_redund: 0.00, num_grp_import: 865, num_grp_redund: 2015\nEp: 60, loss: 0.03, norm_all:1460.31, grp_sparsity: 0.70, acc1: 92.5500, norm_import: 1460.31, norm_redund: 0.00, num_grp_import: 865, num_grp_redund: 2015\nEp: 61, loss: 0.03, norm_all:1459.29, grp_sparsity: 0.70, acc1: 92.5400, norm_import: 1459.29, norm_redund: 0.00, num_grp_import: 865, num_grp_redund: 2015\nEp: 62, loss: 0.02, norm_all:1458.27, grp_sparsity: 0.70, acc1: 92.4900, norm_import: 1458.27, norm_redund: 0.00, num_grp_import: 865, num_grp_redund: 2015\nEp: 63, loss: 0.02, norm_all:1457.25, grp_sparsity: 0.70, acc1: 92.3900, norm_import: 1457.25, norm_redund: 0.00, num_grp_import: 865, num_grp_redund: 2015\nEp: 64, loss: 0.02, norm_all:1456.22, grp_sparsity: 0.70, acc1: 92.6400, norm_import: 1456.22, norm_redund: 0.00, num_grp_import: 865, num_grp_redund: 2015\nEp: 65, loss: 0.02, norm_all:1455.20, grp_sparsity: 0.70, acc1: 92.6600, norm_import: 1455.20, norm_redund: 0.00, num_grp_import: 865, num_grp_redund: 2015\nEp: 66, loss: 0.02, norm_all:1454.18, grp_sparsity: 0.70, acc1: 92.5900, norm_import: 1454.18, norm_redund: 0.00, num_grp_import: 865, num_grp_redund: 2015\nEp: 67, loss: 0.02, norm_all:1453.15, grp_sparsity: 0.70, acc1: 92.7100, norm_import: 1453.15, norm_redund: 0.00, num_grp_import: 865, num_grp_redund: 2015\nEp: 68, loss: 0.02, norm_all:1452.12, grp_sparsity: 0.70, acc1: 92.5900, norm_import: 1452.12, norm_redund: 0.00, num_grp_import: 865, num_grp_redund: 2015\nEp: 69, loss: 0.02, norm_all:1451.10, grp_sparsity: 0.70, acc1: 92.5900, norm_import: 1451.10, norm_redund: 0.00, num_grp_import: 865, num_grp_redund: 2015\nEp: 70, loss: 0.02, norm_all:1450.07, grp_sparsity: 0.70, acc1: 92.6500, norm_import: 1450.07, norm_redund: 0.00, num_grp_import: 865, num_grp_redund: 2015\nEp: 71, loss: 0.02, norm_all:1449.05, grp_sparsity: 0.70, acc1: 92.5800, norm_import: 1449.05, norm_redund: 0.00, num_grp_import: 865, num_grp_redund: 2015\nEp: 72, loss: 0.02, norm_all:1448.02, grp_sparsity: 0.70, acc1: 92.6100, norm_import: 1448.02, norm_redund: 0.00, num_grp_import: 865, num_grp_redund: 2015\nEp: 73, loss: 0.02, norm_all:1446.99, grp_sparsity: 0.70, acc1: 92.6500, norm_import: 1446.99, norm_redund: 0.00, num_grp_import: 865, num_grp_redund: 2015\nEp: 74, loss: 0.02, norm_all:1445.96, grp_sparsity: 0.70, acc1: 92.7100, norm_import: 1445.96, norm_redund: 0.00, num_grp_import: 865, num_grp_redund: 2015\nEp: 75, loss: 0.02, norm_all:1444.93, grp_sparsity: 0.70, acc1: 92.7500, norm_import: 1444.93, norm_redund: 0.00, num_grp_import: 865, num_grp_redund: 2015\nEp: 76, loss: 0.02, norm_all:1443.91, grp_sparsity: 0.70, acc1: 92.8300, norm_import: 1443.91, norm_redund: 0.00, num_grp_import: 865, num_grp_redund: 2015\nEp: 77, loss: 0.02, norm_all:1442.88, grp_sparsity: 0.70, acc1: 92.5500, norm_import: 1442.88, norm_redund: 0.00, num_grp_import: 865, num_grp_redund: 2015\nEp: 78, loss: 0.02, norm_all:1441.85, grp_sparsity: 0.70, acc1: 92.5200, norm_import: 1441.85, norm_redund: 0.00, num_grp_import: 865, num_grp_redund: 2015\nEp: 79, loss: 0.02, norm_all:1440.82, grp_sparsity: 0.70, acc1: 92.3000, norm_import: 1440.82, norm_redund: 0.00, num_grp_import: 865, num_grp_redund: 2015\nEp: 80, loss: 0.01, norm_all:1439.79, grp_sparsity: 0.70, acc1: 92.4100, norm_import: 1439.79, norm_redund: 0.00, num_grp_import: 865, num_grp_redund: 2015\nEp: 81, loss: 0.01, norm_all:1438.76, grp_sparsity: 0.70, acc1: 92.6100, norm_import: 1438.76, norm_redund: 0.00, num_grp_import: 865, num_grp_redund: 2015\nEp: 82, loss: 0.01, norm_all:1437.73, grp_sparsity: 0.70, acc1: 92.7600, norm_import: 1437.73, norm_redund: 0.00, num_grp_import: 865, num_grp_redund: 2015\nEp: 83, loss: 0.01, norm_all:1436.70, grp_sparsity: 0.70, acc1: 92.4800, norm_import: 1436.70, norm_redund: 0.00, num_grp_import: 865, num_grp_redund: 2015\nEp: 84, loss: 0.01, norm_all:1435.67, grp_sparsity: 0.70, acc1: 92.5700, norm_import: 1435.67, norm_redund: 0.00, num_grp_import: 865, num_grp_redund: 2015\nEp: 85, loss: 0.01, norm_all:1434.63, grp_sparsity: 0.70, acc1: 92.6400, norm_import: 1434.63, norm_redund: 0.00, num_grp_import: 865, num_grp_redund: 2015\nEp: 86, loss: 0.01, norm_all:1433.60, grp_sparsity: 0.70, acc1: 92.6900, norm_import: 1433.60, norm_redund: 0.00, num_grp_import: 865, num_grp_redund: 2015\nEp: 87, loss: 0.01, norm_all:1432.57, grp_sparsity: 0.70, acc1: 92.7200, norm_import: 1432.57, norm_redund: 0.00, num_grp_import: 865, num_grp_redund: 2015\nEp: 88, loss: 0.01, norm_all:1431.53, grp_sparsity: 0.70, acc1: 92.5700, norm_import: 1431.53, norm_redund: 0.00, num_grp_import: 865, num_grp_redund: 2015\nEp: 89, loss: 0.01, norm_all:1430.50, grp_sparsity: 0.70, acc1: 92.5500, norm_import: 1430.50, norm_redund: 0.00, num_grp_import: 865, num_grp_redund: 2015\nEp: 90, loss: 0.01, norm_all:1429.47, grp_sparsity: 0.70, acc1: 92.7600, norm_import: 1429.47, norm_redund: 0.00, num_grp_import: 865, num_grp_redund: 2015\nEp: 91, loss: 0.01, norm_all:1428.44, grp_sparsity: 0.70, acc1: 92.6200, norm_import: 1428.44, norm_redund: 0.00, num_grp_import: 865, num_grp_redund: 2015\nEp: 92, loss: 0.01, norm_all:1427.41, grp_sparsity: 0.70, acc1: 92.6800, norm_import: 1427.41, norm_redund: 0.00, num_grp_import: 865, num_grp_redund: 2015\nEp: 93, loss: 0.01, norm_all:1426.38, grp_sparsity: 0.70, acc1: 92.8800, norm_import: 1426.38, norm_redund: 0.00, num_grp_import: 865, num_grp_redund: 2015\nEp: 94, loss: 0.01, norm_all:1425.35, grp_sparsity: 0.70, acc1: 92.6900, norm_import: 1425.35, norm_redund: 0.00, num_grp_import: 865, num_grp_redund: 2015\nEp: 95, loss: 0.01, norm_all:1424.31, grp_sparsity: 0.70, acc1: 92.7500, norm_import: 1424.31, norm_redund: 0.00, num_grp_import: 865, num_grp_redund: 2015\nEp: 96, loss: 0.01, norm_all:1423.29, grp_sparsity: 0.70, acc1: 92.7000, norm_import: 1423.29, norm_redund: 0.00, num_grp_import: 865, num_grp_redund: 2015\nEp: 97, loss: 0.01, norm_all:1422.26, grp_sparsity: 0.70, acc1: 92.7300, norm_import: 1422.26, norm_redund: 0.00, num_grp_import: 865, num_grp_redund: 2015\nEp: 98, loss: 0.01, norm_all:1421.24, grp_sparsity: 0.70, acc1: 92.6600, norm_import: 1421.24, norm_redund: 0.00, num_grp_import: 865, num_grp_redund: 2015\nEp: 99, loss: 0.01, norm_all:1421.13, grp_sparsity: 0.70, acc1: 92.6000, norm_import: 1421.13, norm_redund: 0.00, num_grp_import: 865, num_grp_redund: 2015\n","output_type":"stream"}],"execution_count":9},{"id":"cb8dabf9-04cf-4d55-89e4-a17c96d17cec","cell_type":"markdown","source":"### Step 5. Get compressed model in torch format","metadata":{}},{"id":"f46e6b6e-6286-4f57-816f-f94f6ce5c40b","cell_type":"code","source":"# By default OTO will construct subnet by the last checkpoint. If intermedia ckpt reaches the best performance,\n# need to reinitialize OTO instance\n# oto = OTO(torch.load(ckpt_path), dummy_input)\n# then construct subnetwork\noto.construct_subnet(out_dir='./cache')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T16:32:42.900229Z","iopub.execute_input":"2025-07-16T16:32:42.900516Z","iopub.status.idle":"2025-07-16T16:32:42.985770Z","shell.execute_reply.started":"2025-07-16T16:32:42.900488Z","shell.execute_reply":"2025-07-16T16:32:42.984706Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/1330004468.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# oto = OTO(torch.load(ckpt_path), dummy_input)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# then construct subnetwork\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0moto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstruct_subnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'./cache'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'oto' is not defined"],"ename":"NameError","evalue":"name 'oto' is not defined","output_type":"error"}],"execution_count":1},{"id":"da9cd3b6-1db9-473d-9585-27e08c669265","cell_type":"markdown","source":"### (Optional) Check the compressed model size","metadata":{}},{"id":"dc731c99-3155-472e-9462-65ed8c9bf4d7","cell_type":"code","source":"import os\n\nfull_model_size = os.stat(oto.full_group_sparse_model_path)\ncompressed_model_size = os.stat(oto.compressed_model_path)\nprint(\"Size of full model     : \", full_model_size.st_size / (1024 ** 3), \"GBs\")\nprint(\"Size of compress model : \", compressed_model_size.st_size / (1024 ** 3), \"GBs\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-16T16:32:15.985Z"}},"outputs":[],"execution_count":null},{"id":"4c6d825e-46c8-49e9-b9f0-6f4eea4d8e2d","cell_type":"markdown","source":"### (Optional) Check the compressed model accuracy\n#### # Both full and compressed model should return the exact same accuracy.","metadata":{}},{"id":"be56f693-3566-4b91-b5a7-39f327e61450","cell_type":"code","source":"full_model = torch.load(oto.full_group_sparse_model_path)\ncompressed_model = torch.load(oto.compressed_model_path)\n\nacc1_full, acc5_full = check_accuracy(full_model, testloader)\nprint(\"Full model: Acc 1: {acc1}, Acc 5: {acc5}\".format(acc1=acc1_full, acc5=acc5_full))\n\nacc1_compressed, acc5_compressed = check_accuracy(compressed_model, testloader)\nprint(\"Compressed model: Acc 1: {acc1}, Acc 5: {acc5}\".format(acc1=acc1_compressed, acc5=acc5_compressed))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T15:54:20.545231Z","iopub.execute_input":"2025-07-16T15:54:20.545392Z","iopub.status.idle":"2025-07-16T15:54:24.874147Z","shell.execute_reply.started":"2025-07-16T15:54:20.545378Z","shell.execute_reply":"2025-07-16T15:54:24.873350Z"}},"outputs":[{"name":"stdout","text":"Full model: Acc 1: 92.6, Acc 5: 99.74\nCompressed model: Acc 1: 92.6, Acc 5: 99.74\n","output_type":"stream"}],"execution_count":12}]}